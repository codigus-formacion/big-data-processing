{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e85b024c",
      "metadata": {
        "id": "e85b024c"
      },
      "source": [
        "# RDDs con PySpark\n",
        "\n",
        "En este notebook aprenderemos cómo crear una sesión de Spark (SparkSession) y cómo crear RDDs a partir de colecciones de Python y de ficheros externos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c659004",
      "metadata": {},
      "source": [
        "Para poder ejecutar Spark es necesario disponer de una instalación de Java. Se puede comprobar si Java está instalado en la máquina con el siguiente comando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6069b1dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "!java --version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a7dd4e",
      "metadata": {},
      "source": [
        "En caso de que el comando anterior falle, será necesario instalar Java en la máquina. Podemos instalar la versión LTS de Java 21 con el siguiente comando si estamos en Google Colab o en una máquina con Linux. Para instalar Java en Windows, tendremos que visitar la siguiente página (https://www.oracle.com/es/java/technologies/downloads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32d399a",
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt install -y openjdk-21-jdk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edac01fe",
      "metadata": {},
      "source": [
        "A continuación, será necesario comprobar si la variable `JAVA_HOME`, que indica a Spark dónde encontrar la JVM para ejecutar su código Scala, está definida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d424b0ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780c448e",
      "metadata": {},
      "source": [
        "En caso de que la variable no esté definida, se debe establecer de la siguiente manera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9344deeb",
      "metadata": {
        "id": "9344deeb"
      },
      "outputs": [],
      "source": [
        "# En nuestro ordenador personal, si no esta definida la variable JAVA_HOME, deberemos indicarla\n",
        "# Para sistemas basados en Debian/Ubuntu, si tenemos instalada la version 21 de Java, seria:\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-21-openjdk-amd64\"\n",
        "# En Windows, la ruta puede ser algo como: \"C:\\\\Program Files\\\\Java\\\\jdk-21\"\n",
        "# os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-21\"\n",
        "# Si ya esta definida, no es necesario hacer nada\n",
        "# os.environ[\"JAVA_HOME\"]\n",
        "\n",
        "# En Google Colab no es necesario hacer nada"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a7a780e",
      "metadata": {
        "id": "6a7a780e"
      },
      "source": [
        "A continuación, importaremos PySpark y verificaremos si está correctamente instalado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d05c8f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7d05c8f3",
        "outputId": "245b67d8-3c7e-44e8-c367-7d93ff7ca778"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4.0.1'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "\n",
        "pyspark.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f47449",
      "metadata": {},
      "source": [
        "En caso de que PySpark no esté instalado, se debe instalar con el siguiente comando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760f6c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "import pyspark\n",
        "pyspark.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db02c8e",
      "metadata": {
        "id": "9db02c8e"
      },
      "source": [
        "Ya estamos listos para crear una sesión de Spark y empezar a trabajar con RDDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "26543cc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Crear sesión de Spark\n",
        "spark = SparkSession.builder.appName(\"Analisis Deportivo con Spark\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "660479a8",
      "metadata": {
        "id": "660479a8"
      },
      "source": [
        "Una vez creado un objeto `SparkSession`, podemos inspeccionarlo. Ahora mismo lo estamos utilizando en modo local, con todos los núcleos de nuestro ordenador -> [*]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "58d11597",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "58d11597",
        "outputId": "2c75ced1-eeb5-49ec-8146-accccaa6e7e8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10.0.68.169:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v4.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Analisis Deportivo con Spark</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x74df930cf640>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ccd26c",
      "metadata": {
        "id": "62ccd26c"
      },
      "source": [
        "### Ejemplo 1\n",
        "\n",
        "Vamos a crear un RDD a partir de una colección de Python. En este caso, una lista con información sobre jugadores y su distancia recorrida en un partido de fútbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e3643e6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3643e6e",
        "outputId": "01cc621b-1c82-4237-f8d8-09c8ed85f33d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Jugador2', 9.8), ('Jugador1', 21.2)]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Datos simulados: (jugador, distancia recorrida en km)\n",
        "football_data = [(\"Jugador1\", 10.2), (\"Jugador2\", 9.8), (\"Jugador1\", 11.0)]\n",
        "\n",
        "# Creamos un RDD a partir de la lista\n",
        "rdd = spark.sparkContext.parallelize(football_data)\n",
        "\n",
        "# Distancia total por jugador\n",
        "total_distance = rdd.reduceByKey(lambda a, b: a + b)\n",
        "total_distance.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "691cb03b",
      "metadata": {
        "id": "691cb03b"
      },
      "source": [
        "### Ejemplo 2\n",
        "\n",
        "Veremos una implementación del clásico problema de contar palabras (word count) utilizando RDDs. En este caso, el RDD se crea a partir de un fichero de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v6h-J37ayg9h",
      "metadata": {
        "id": "v6h-J37ayg9h"
      },
      "source": [
        "Si estamos trabajando en local con Jupyter Notebook/Lab, accedemos directamente utilizando la carpeta `data/` (deberás comentar las opciones de Google Collab para que no sobrescriba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "BQof2IubyLcp",
      "metadata": {
        "id": "BQof2IubyLcp"
      },
      "outputs": [],
      "source": [
        "file_path = \"data/wordcount_data.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6T9YYoHvlZY",
      "metadata": {
        "id": "a6T9YYoHvlZY"
      },
      "source": [
        "Si estamos en Google Collab tenemos 2 opciones:\n",
        "\n",
        "**Opción 1: Subir un fichero de datos a mano:**\n",
        "- Abrirmos el directorio actual (:file_folder:) y arrastramos el fichero que queremos utilizar (wordcount_data.txt)\n",
        "- Cuando cerremos Google Colab, ese fichero se _perderá_\n",
        "- El fichero estará disponible usando simplemente su nombre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gHGHYescvwa0",
      "metadata": {
        "id": "gHGHYescvwa0"
      },
      "outputs": [],
      "source": [
        "file_path = \"wordcount_data.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9zAd6RFrwMn_",
      "metadata": {
        "id": "9zAd6RFrwMn_"
      },
      "source": [
        "**Opción 2: Montamos una carpeta de Google Drive que contenga nuestros datos**\n",
        "- Deberemos tener creada una carpeta \"Colab Notebooks\" en la raiz de nuestra carpeta de Google Drive (se crea sola al subir el primer Notebook)\n",
        "- Subimos nuestros datos (por ejemplo, la carpeta `data/` que tenemos en los ejemplos de Spark)\n",
        "- Para tener las rutas más limpias, creamos un enlace simbólico para que los datos estén disponibles en la carpeta workspace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "912d185f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Montamos la carpeta (nos pedirá permisos)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Crea un atajo llamado 'workspace' en la carpeta /content (dará un pequeño error si ya existe)\n",
        "!ln -s \"/content/drive/MyDrive/Colab Notebooks\" \"/content/workspace\" >/dev/null 2>&1\n",
        "# Ya podemos acceder a los ficheros, por ejemplo:\n",
        "file_path = \"/content/workspace/data/wordcount_data.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d505bf1a",
      "metadata": {
        "id": "d505bf1a"
      },
      "outputs": [],
      "source": [
        "rdd = spark.sparkContext.textFile(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0f05933",
      "metadata": {},
      "source": [
        "Mostramos el número de líneas del fichero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "4886b985",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4886b985",
        "outputId": "7cb66560-d7f0-400d-eb5f-3f4858eac2cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd.count() # Numero de lineas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "aaaaba30",
      "metadata": {
        "id": "aaaaba30"
      },
      "outputs": [],
      "source": [
        "rdd_word_count =(rdd\n",
        "    .flatMap(lambda line: line.split())\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(lambda x, y: x + y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ac8d266e",
      "metadata": {
        "id": "ac8d266e"
      },
      "outputs": [],
      "source": [
        "rdd_word_count.saveAsTextFile(\"output_wordcount\") # Si ya existe el directorio, da error, podemos ejecutar en la terminal rm -rf output_wordcount/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dfe80cc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfe80cc4",
        "outputId": "6c02777f-9654-464d-bad7-60e80473d6d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('the', 38),\n",
              " ('a', 28),\n",
              " ('of', 25),\n",
              " ('word', 24),\n",
              " ('and', 23),\n",
              " ('words', 21),\n",
              " ('is', 19),\n",
              " ('to', 18),\n",
              " ('count', 11),\n",
              " ('in', 11)]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_word_count.takeOrdered(10, key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ca8c7e",
      "metadata": {
        "id": "b9ca8c7e"
      },
      "source": [
        "Finalmente, podemos cerrar la sesión de Spark cuando ya no la necesitemos. Si lo hacemos, habrá que ejecutar todo el notebook de nuevo para volver a crearla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5de5f437",
      "metadata": {
        "id": "5de5f437"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
